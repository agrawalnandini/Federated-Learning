{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib\n",
    "#pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import syft as sy\n",
    "from syft.frameworks.torch.fl import utils\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112207650>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.epochs = 20\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 1000\n",
    "        self.batch_size = 64\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)  \n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
    "nodes=[bob,alice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=False,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    #.federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    ,batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "#sy.FederatedDataLoader- when we were using federate \n",
    "#note one (x,y) is : number of x's in one are based on batch size - same way for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(), list())\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(nodes[batch_idx % len(nodes)])# can send using .federate directly and using FederatedDataLoader but that was giving issues so sending manually\n",
    "    target = target.send(nodes[batch_idx % len(nodes)])\n",
    "    remote_dataset[batch_idx % len(nodes)].append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remote_dataset[0])\n",
    "#x,y=remote_dataset[0][0]\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "bobs_optimizer = optim.Adam(bobs_model.parameters(), lr=args.lr)\n",
    "alices_optimizer = optim.Adam(alices_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net()\n",
    "print(global_model)\n",
    "weights=global_model.fc2.weight.data  #gives last layer weights before softmax output\n",
    "print(weights)\n",
    "#plt.plot(weights)\n",
    "#plt.show()\n",
    "w = list(global_model.parameters()) #gives all parameters(weights of all)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(x, y, model, optimizer):\n",
    "    model.send(x.location)#sending to correct location\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(x)\n",
    "    loss = F.nll_loss(prediction, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    s = torch.sum(model.fc2.weight.data)\n",
    "    print(s)\n",
    "    return model\n",
    "\n",
    "def train():\n",
    "    for data_index in range(len(remote_dataset[0])-1):# 2 datasets (one w bob and other w alice)\n",
    "        for worker_index in range(len(nodes)):\n",
    "            x, y = remote_dataset[worker_index][data_index]\n",
    "            models[worker_index] = update(x, y, models[worker_index], optimizers[worker_index])\n",
    "        for model in models:\n",
    "            model.get()\n",
    "        #print(models[0])\n",
    "        return utils.federated_avg({\n",
    "            \"bob\": models[0],\n",
    "            \"alice\": models[1]# securely aggregate both models and return global model\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights_bob=models[0].fc2.weight.data  #last layer weights bob\n",
    "plt.plot(trained_weights_bob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(federated_model):\n",
    "    federated_model.eval()\n",
    "    test_loss = 0\n",
    "    correct=0\n",
    "    for x, y in test_loader:\n",
    "        output = federated_model(x)\n",
    "        test_loss += F.nll_loss(output, y, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "        #print(y.view_as(pred))\n",
    "        #print(pred)\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        #print(correct)\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)#this is total number of images in test data, whereas test_loader is z images grouped together depending on batch size,\n",
    "    #so if batch size is 10 then 10 x's and 10 corresponding y's will go together thus y vector is like that\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format\n",
    "          (test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "    #print('Test set: Average loss: {:.4f}'.format(test_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number 1\n",
      "(Wrapper)>[PointerTensor | me:82276082999 -> bob:98796756249]\n",
      "(Wrapper)>[PointerTensor | me:42106527622 -> alice:58915896887]\n",
      "\n",
      "Test set: Average loss: 2.2822, Accuracy: 1855/10000 (19%)\n",
      "\n",
      "Epoch Number 2\n",
      "(Wrapper)>[PointerTensor | me:60750013013 -> bob:76193917730]\n",
      "(Wrapper)>[PointerTensor | me:39717693624 -> alice:4279880527]\n",
      "\n",
      "Test set: Average loss: 2.1763, Accuracy: 3096/10000 (31%)\n",
      "\n",
      "Epoch Number 3\n",
      "(Wrapper)>[PointerTensor | me:26378459524 -> bob:42818772738]\n",
      "(Wrapper)>[PointerTensor | me:69514194504 -> alice:62145568709]\n",
      "\n",
      "Test set: Average loss: 1.9386, Accuracy: 4072/10000 (41%)\n",
      "\n",
      "Epoch Number 4\n",
      "(Wrapper)>[PointerTensor | me:86651289767 -> bob:51565051279]\n",
      "(Wrapper)>[PointerTensor | me:40687139788 -> alice:85367152746]\n",
      "\n",
      "Test set: Average loss: 1.6433, Accuracy: 4981/10000 (50%)\n",
      "\n",
      "Epoch Number 5\n",
      "(Wrapper)>[PointerTensor | me:19874933939 -> bob:97110431828]\n",
      "(Wrapper)>[PointerTensor | me:17167280398 -> alice:54964575391]\n",
      "\n",
      "Test set: Average loss: 1.3595, Accuracy: 6247/10000 (62%)\n",
      "\n",
      "Epoch Number 6\n",
      "(Wrapper)>[PointerTensor | me:21230717363 -> bob:54151670415]\n",
      "(Wrapper)>[PointerTensor | me:13167364781 -> alice:44704422688]\n",
      "\n",
      "Test set: Average loss: 1.1337, Accuracy: 6874/10000 (69%)\n",
      "\n",
      "Epoch Number 7\n",
      "(Wrapper)>[PointerTensor | me:14046055883 -> bob:19996867743]\n",
      "(Wrapper)>[PointerTensor | me:25523341212 -> alice:36609444508]\n",
      "\n",
      "Test set: Average loss: 1.0025, Accuracy: 6976/10000 (70%)\n",
      "\n",
      "Epoch Number 8\n",
      "(Wrapper)>[PointerTensor | me:60945806780 -> bob:71835366227]\n",
      "(Wrapper)>[PointerTensor | me:3958552201 -> alice:2848569687]\n",
      "\n",
      "Test set: Average loss: 0.9301, Accuracy: 7009/10000 (70%)\n",
      "\n",
      "Epoch Number 9\n",
      "(Wrapper)>[PointerTensor | me:10454913731 -> bob:12611380106]\n",
      "(Wrapper)>[PointerTensor | me:52258491385 -> alice:89743081260]\n",
      "\n",
      "Test set: Average loss: 0.8836, Accuracy: 7071/10000 (71%)\n",
      "\n",
      "Epoch Number 10\n",
      "(Wrapper)>[PointerTensor | me:70204995401 -> bob:68766391874]\n",
      "(Wrapper)>[PointerTensor | me:33650236610 -> alice:88047116759]\n",
      "\n",
      "Test set: Average loss: 0.8455, Accuracy: 7209/10000 (72%)\n",
      "\n",
      "Epoch Number 11\n",
      "(Wrapper)>[PointerTensor | me:83160005808 -> bob:30090132444]\n",
      "(Wrapper)>[PointerTensor | me:5948084116 -> alice:21189889298]\n",
      "\n",
      "Test set: Average loss: 0.7969, Accuracy: 7374/10000 (74%)\n",
      "\n",
      "Epoch Number 12\n",
      "(Wrapper)>[PointerTensor | me:5323243985 -> bob:22425601894]\n",
      "(Wrapper)>[PointerTensor | me:28306277192 -> alice:41592001892]\n",
      "\n",
      "Test set: Average loss: 0.7596, Accuracy: 7569/10000 (76%)\n",
      "\n",
      "Epoch Number 13\n",
      "(Wrapper)>[PointerTensor | me:52377111534 -> bob:54719302637]\n",
      "(Wrapper)>[PointerTensor | me:60432740041 -> alice:63887164057]\n",
      "\n",
      "Test set: Average loss: 0.7357, Accuracy: 7681/10000 (77%)\n",
      "\n",
      "Epoch Number 14\n",
      "(Wrapper)>[PointerTensor | me:81391063956 -> bob:66582221670]\n",
      "(Wrapper)>[PointerTensor | me:80460331748 -> alice:63015653443]\n",
      "\n",
      "Test set: Average loss: 0.7230, Accuracy: 7720/10000 (77%)\n",
      "\n",
      "Epoch Number 15\n",
      "(Wrapper)>[PointerTensor | me:40785280514 -> bob:61409024478]\n",
      "(Wrapper)>[PointerTensor | me:88085630181 -> alice:421268184]\n",
      "\n",
      "Test set: Average loss: 0.7246, Accuracy: 7737/10000 (77%)\n",
      "\n",
      "Epoch Number 16\n",
      "(Wrapper)>[PointerTensor | me:50600526780 -> bob:97029118977]\n",
      "(Wrapper)>[PointerTensor | me:55857294673 -> alice:33775035634]\n",
      "\n",
      "Test set: Average loss: 0.7413, Accuracy: 7716/10000 (77%)\n",
      "\n",
      "Epoch Number 17\n",
      "(Wrapper)>[PointerTensor | me:39069376834 -> bob:27534477210]\n",
      "(Wrapper)>[PointerTensor | me:7079791027 -> alice:93739688522]\n",
      "\n",
      "Test set: Average loss: 0.7585, Accuracy: 7715/10000 (77%)\n",
      "\n",
      "Epoch Number 18\n",
      "(Wrapper)>[PointerTensor | me:50007887441 -> bob:73903642583]\n",
      "(Wrapper)>[PointerTensor | me:3998647022 -> alice:32111417768]\n",
      "\n",
      "Test set: Average loss: 0.7681, Accuracy: 7730/10000 (77%)\n",
      "\n",
      "Epoch Number 19\n",
      "(Wrapper)>[PointerTensor | me:92393178639 -> bob:92761534899]\n",
      "(Wrapper)>[PointerTensor | me:95199926580 -> alice:91443917019]\n",
      "\n",
      "Test set: Average loss: 0.7764, Accuracy: 7764/10000 (78%)\n",
      "\n",
      "Epoch Number 20\n",
      "(Wrapper)>[PointerTensor | me:3841413056 -> bob:15622072321]\n",
      "(Wrapper)>[PointerTensor | me:97699958971 -> alice:75238519517]\n",
      "\n",
      "Test set: Average loss: 0.7859, Accuracy: 7782/10000 (78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    print(f\"Epoch Number {epoch + 1}\")\n",
    "    federated_model = train()\n",
    "    global_model = federated_model\n",
    "    test(federated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
