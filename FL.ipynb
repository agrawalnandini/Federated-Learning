{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib\n",
    "#pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jupyter-tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import syft as sy\n",
    "from syft.frameworks.torch.fl import utils\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter \n",
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12173e2f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.epochs = 20\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 1000\n",
    "        self.batch_size = 64\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)  \n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
    "nodes=[bob,alice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=False,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "    #.federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    ,batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "#sy.FederatedDataLoader- when we were using federate \n",
    "#note one (x,y) is : number of x's in one are based on batch size - same way for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader.dataset)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(), list())\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(nodes[batch_idx % len(nodes)])# can send using .federate directly and using FederatedDataLoader but that was giving issues so sending manually\n",
    "    target = target.send(nodes[batch_idx % len(nodes)])\n",
    "    remote_dataset[batch_idx % len(nodes)].append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remote_dataset[0]) #numbr of batches with Bob and same for Alice\n",
    "#len(remote_dataset[0][468])\n",
    "#x,y=remote_dataset[0][0]\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "bobs_optimizer = optim.Adam(bobs_model.parameters(), lr=args.lr)\n",
    "alices_optimizer = optim.Adam(alices_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net()\n",
    "print(global_model)\n",
    "\n",
    "\n",
    "#weights=global_model.fc2.weight.data  #gives last layer weights before softmax output\n",
    "#print(weights)\n",
    "#plt.plot(weights)\n",
    "#plt.show()\n",
    "#w = list(global_model.parameters()) #gives all parameters(weights of all)\n",
    "#print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(model):\n",
    "    model_weights=model.fc2.weight\n",
    "    #print(model_weights)\n",
    "    list_weights=model_weights.tolist()\n",
    "    weights = [item for sublist in list_weights for item in sublist]#flattened list\n",
    "    plt.hist(weights,15)\n",
    "    plt.show()\n",
    "        \n",
    "def update(x, y, model, optimizer):\n",
    "    model.send(x.location)#sending to correct location\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(x)\n",
    "    loss = F.nll_loss(prediction, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "def train(count):\n",
    "    for data_index in range(count,40+count): #training with 40 batches per epoch\n",
    "        for worker_index in range(len(nodes)):\n",
    "            x, y = remote_dataset[worker_index][data_index]# 2 datasets (one w bob and other w alice)\n",
    "            models[worker_index] = update(x, y, models[worker_index], optimizers[worker_index])\n",
    "        for model in models:\n",
    "            model.get()\n",
    "    \n",
    "    plot_weights(models[0])\n",
    "    plot_weights(models[1])\n",
    "    return utils.federated_avg({\n",
    "    \"bob\": models[0],\n",
    "    \"alice\": models[1]# securely aggregate both models and return global model\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_weights_bob=models[0].fc2.weight.data  #last layer weights bob\n",
    "#plt.plot(trained_weights_bob)\n",
    "#plt.show()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(federated_model):\n",
    "    federated_model.eval()\n",
    "    test_loss = 0\n",
    "    correct=0\n",
    "    for x, y in test_loader:\n",
    "        output = federated_model(x)\n",
    "        test_loss += F.nll_loss(output, y, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "        #print(y.view_as(pred))\n",
    "        #print(pred)\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "        #print(correct)\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)#this is total number of images in test data, whereas test_loader is z images grouped together depending on batch size,\n",
    "    #so if batch size is 10 then 10 x's and 10 corresponding y's will go together thus y vector is like that\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format\n",
    "          (test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "    #print('Test set: Average loss: {:.4f}'.format(test_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training process\n",
    "i=0\n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"Epoch Number {epoch + 1}\")\n",
    "    federated_model = train(i)\n",
    "    global_model = federated_model\n",
    "    i+=40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "test(global_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
